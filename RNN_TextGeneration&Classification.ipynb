{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CMPT 980 - Assignment 3\n",
    "\n",
    "Puria Azadi Moghadam - Student No: 301406080"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZzz1jkW5MRj"
   },
   "source": [
    "## Deep Learning Course (980)\n",
    "## Assignment Three \n",
    "\n",
    "__Assignment Goals:__\n",
    "\n",
    "- Implementing RNN based language models.\n",
    "- Implementing and applying a Recurrent Neural Network on text classification problem using TensorFlow.\n",
    "- Implementing __many to one__ and __many to many__ RNN sequence processing.\n",
    "\n",
    "In this assignment, you will implement RNN-based language models and compare extracted word representation from different models. You will also compare two different training methods for sequential data: Truncated Backpropagation Through Time __(TBTT)__ and Backpropagation Through Time __(BTT)__. \n",
    "Also, you will be asked to apply Vanilla RNN to capture word representations and solve a text classification problem. \n",
    "\n",
    "\n",
    "__DataSets__: You will use two datasets, an English Literature for language model task (part 1 to 4) and 20Newsgroups for text classification (part 5). \n",
    "\n",
    "\n",
    "1. (30 points) Implement the RNN based language model described by Mikolov et al.[1], also called __Elman network__ and train a language model on the English Literature dataset. This network contains input, hidden and output layer and is trained by standard backpropagation (TBTT with τ = 1) using the cross-entropy loss. \n",
    "   - The input represents the current word while using 1-of-N coding (thus its size is equal to the size of the vocabulary) and vector s(t − 1) that represents output values in the hidden layer from the previous time step. \n",
    "   - The hidden layer is a fully connected sigmoid layer with size 500. \n",
    "   - Softmax Output Layer to capture a valid probability distribution.\n",
    "   - The model is trained with truncated backpropagation through time (TBTT) with τ = 1: the weights of the network are updated based on the error vector computed only for the current time step.\n",
    "   \n",
    "   Download the English Literature dataset and train the language model as described, report the model cross-entropy loss on the train set. Use nltk.word_tokenize to tokenize the documents. \n",
    "For initialization, s(0) can be set to a vector of small values. Note that we are not interested in the *dynamic model* mentioned in the original paper. \n",
    "To make the implementation simpler you can use Keras to define neural net layers, including Keras.Embedding. (Keras.Embedding will create an additional mapping layer compared to the Elman architecture.) \n",
    "\n",
    "2. (20 points) TBTT has less computational cost and memory needs in comparison with *backpropagation through time algorithm (BTT)*. These benefits come at the cost of losing long term dependencies [2]. Now let's try to investigate computational costs and performance of learning our language model with BTT. For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (The input  size will be [1, Sentence Length]). \n",
    "\n",
    "    1. Split the document into sentences (you can use nltk.tokenize.sent_tokenize).\n",
    "    2. For each sentence, perform one pass that computes the mean/sum loss for this sentence; then perform a gradient update for the whole sentence. (So the mini-batch size varies for the sentences with different lengths). You can truncate long sentences to fit the data in memory. \n",
    "    3. Report the model cross-entropy loss.\n",
    "\n",
    "3. (15 points) It does not seem that simple recurrent neural networks can capture truly exploit context information with long dependencies, because of the problem that gradients vanish and exploding. To solve this problem, gating mechanisms for recurrent neural networks were introduced. Try to learn your last model (Elman + BTT) with the SimpleRnn unit replaced with a Gated Recurrent Unit (GRU). Report the model cross-entropy loss. Compare your results in terms of cross-entropy loss with two other approach(part 1 and 2). Use each model to generate 10 synthetic sentences of 15 words each. Discuss the quality of the sentences generated - do they look like proper English? Do they match the training set?\n",
    "    Text generation from a given language model can be done using the following iterative process:\n",
    "   1. Set sequence = \\[first_word\\], chosen randomly.\n",
    "   2. Select a new word based on the sequence so far, add this word to the sequence, and repeat. At each iteration, select the word with maximum probability given the sequence so far. The trained language model outputs this probability. \n",
    "\n",
    "4. (15 points) The text describes how to extract a word representation from a trained RNN (Chapter 4). How we can evaluate the extracted word representation for your trained RNN? Compare the words representation extracted from each of the approaches using one of the existing methods.\n",
    "\n",
    "5. (20 points) We are aiming to learn an RNN model that predicts document categories given its content (text classification). For this task, we will use the 20Newsgroupst dataset. The 20Newsgroupst contains messages from twenty newsgroups.  We selected four major categories (comp, politics, rec, and religion) comprising around 13k documents altogether. Your model should learn word representations to support the classification task. For solving this problem modify the __Elman network__ architecture such that the last layer is a softmax layer with just 4 output neurons (one for each category). \n",
    "\n",
    "    1. Download the 20Newsgroups dataset, and use the implemented code from the notebook to read in the dataset.\n",
    "    2. Split the data into a training set (90 percent) and validation set (10 percent). Train the model on  20Newsgroups.\n",
    "    3. Report your accuracy results on the validation set.\n",
    "\n",
    "__NOTE__: Please use Jupyter Notebook. The notebook should include the final code, results and your answers. You should submit your Notebook in (.pdf or .html) and .ipynb format. (penalty 10 points) \n",
    "\n",
    "To reduce the parameters, you can merge all words that occur less often than a threshold into a special rare token (\\__unk__).\n",
    "\n",
    "__Instructions__:\n",
    "\n",
    "The university policy on academic dishonesty and plagiarism (cheating) will be taken very seriously in this course. Everything submitted should be your own writing or coding. You must not let other students copy your work. Spelling and grammar count.\n",
    "\n",
    "Your assignments will be marked based on correctness, originality (the implementations and ideas are from yourself), clarification and test performance.\n",
    "\n",
    "\n",
    "[1] Tom´ as Mikolov, Martin Kara ˇ fiat, Luk´ ´ as Burget, Jan ˇ Cernock´ ˇ y,Sanjeev Khudanpur: Recurrent neural network based language model, In: Proc. INTERSPEECH 2010\n",
    "\n",
    "[2] Tallec, Corentin, and Yann Ollivier. \"Unbiasing truncated backpropagation through time.\" arXiv preprint arXiv:1705.08209 (2017).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rrlEQvm5HYb"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 117
    },
    "colab_type": "code",
    "id": "Scu-vv0t5c2c",
    "outputId": "13481512-3f93-420b-e00e-16cb8789577f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk \n",
    "nltk.download('punkt')\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Masking, SimpleRNN, TimeDistributed, LSTM, GRU\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "eRMxuei35mIq",
    "outputId": "3bd3fa07-cfb9-4470-8618-2158b29fd713"
   },
   "outputs": [],
   "source": [
    "with open(\"./EnglishLiterature.txt\") as f:\n",
    "    #content = f.readlines()\n",
    "    content = [content.rstrip() for content in f]\n",
    "    #print(content)\n",
    "\n",
    "#print(content)\n",
    "content2=[s for s in content if s != '']\n",
    "# print(content2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 55
    },
    "colab_type": "code",
    "id": "Dp9MgN-56PCg",
    "outputId": "abe40398-d4d4-4c68-ea1f-c5b32f1aaa44"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(content2)\n",
    "#print(content2)\n",
    "all_integer_sentences=[]\n",
    "for line in content2:\n",
    "  for word in line.split():\n",
    "    all_integer_sentences.append(word)\n",
    "all_integer_sentences4= [tokenizer.texts_to_sequences([line])[0] for line in content2]\n",
    "#all_integer_sentences = tokenizer.texts_to_sequences([content2])\n",
    "all_integer_sentences2 = tokenizer.texts_to_sequences(all_integer_sentences)\n",
    "# print(all_integer_sentences2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0v6wEX313zL"
   },
   "source": [
    "## **PART ONE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 126
    },
    "colab_type": "code",
    "id": "F_9L5dqz6Zhn",
    "outputId": "86965961-b0f0-48c3-92d7-ebb4bec7cfde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(202648,)\n",
      "12633\n",
      "(202648,)\n",
      "(202648, 12633)\n"
     ]
    }
   ],
   "source": [
    "inputs=[]\n",
    "outputs=[]\n",
    "for integer_sentence in all_integer_sentences:\n",
    "    for i in range(len(integer_sentence)-1):\n",
    "        inputs.append(integer_sentence[i:i+1])\n",
    "        outputs.append(integer_sentence[i+1:i+2])\n",
    "\n",
    "all_integer_sentences2 =  [integer[0] for integer in all_integer_sentences2 if integer != []]\n",
    "inputs = all_integer_sentences2[0:-1]\n",
    "outputs = all_integer_sentences2[1:]\n",
    "print(np.shape(inputs))\n",
    "# print((outputs))\n",
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word\n",
    "num_words = len(word_idx) + 1\n",
    "inputs=np.array(inputs)\n",
    "outputs=np.array(outputs)\n",
    "print(num_words)\n",
    "categorical_outputs = to_categorical(outputs, num_classes=num_words)\n",
    "print(np.shape(inputs))\n",
    "print(np.shape(categorical_outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "colab_type": "code",
    "id": "3MeomGR76hSW",
    "outputId": "0f3dda39-42f9-4dd6-e417-4e204d6bbe8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 50)          631650    \n",
      "_________________________________________________________________\n",
      "simple_rnn_1 (SimpleRNN)     (None, 500)               275500    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 12633)             6329133   \n",
      "=================================================================\n",
      "Total params: 7,236,283\n",
      "Trainable params: 7,236,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(num_words, 50, input_length=None))\n",
    "model.add(SimpleRNN(500, return_sequences=False, activation='sigmoid'))\n",
    "# model.add(SimpleRNN(500, activation='sigmoid'))\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "model.add(Dense(num_words, activation='softmax'))\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "_2oxcYqk6j3e",
    "outputId": "6dd2c545-078a-4996-d5a9-f533a7e5742b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "Epoch 1/20\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
      "\n",
      "202648/202648 [==============================] - 62s 304us/step - loss: 6.9747 - acc: 0.0408\n",
      "Epoch 2/20\n",
      "202648/202648 [==============================] - 52s 257us/step - loss: 6.4280 - acc: 0.0706\n",
      "Epoch 3/20\n",
      "202648/202648 [==============================] - 52s 259us/step - loss: 6.1886 - acc: 0.0849\n",
      "Epoch 4/20\n",
      "202648/202648 [==============================] - 53s 260us/step - loss: 6.0234 - acc: 0.0934\n",
      "Epoch 5/20\n",
      "202648/202648 [==============================] - 52s 259us/step - loss: 5.8932 - acc: 0.1004\n",
      "Epoch 6/20\n",
      "202648/202648 [==============================] - 52s 258us/step - loss: 5.7787 - acc: 0.1055\n",
      "Epoch 7/20\n",
      "202648/202648 [==============================] - 52s 258us/step - loss: 5.6712 - acc: 0.1105\n",
      "Epoch 8/20\n",
      "202648/202648 [==============================] - 53s 260us/step - loss: 5.5719 - acc: 0.1149\n",
      "Epoch 9/20\n",
      "202648/202648 [==============================] - 53s 260us/step - loss: 5.4766 - acc: 0.1181\n",
      "Epoch 10/20\n",
      "202648/202648 [==============================] - 53s 260us/step - loss: 5.3848 - acc: 0.1207\n",
      "Epoch 11/20\n",
      "202648/202648 [==============================] - 52s 259us/step - loss: 5.2955 - acc: 0.1234\n",
      "Epoch 12/20\n",
      "202648/202648 [==============================] - 52s 258us/step - loss: 5.2087 - acc: 0.1253\n",
      "Epoch 13/20\n",
      "202648/202648 [==============================] - 52s 259us/step - loss: 5.1276 - acc: 0.1262\n",
      "Epoch 14/20\n",
      "202648/202648 [==============================] - 52s 258us/step - loss: 5.0552 - acc: 0.1282\n",
      "Epoch 15/20\n",
      "202648/202648 [==============================] - 52s 259us/step - loss: 4.9905 - acc: 0.1286\n",
      "Epoch 16/20\n",
      "202648/202648 [==============================] - 53s 259us/step - loss: 4.9364 - acc: 0.1297\n",
      "Epoch 17/20\n",
      "202648/202648 [==============================] - 52s 258us/step - loss: 4.8903 - acc: 0.1301\n",
      "Epoch 18/20\n",
      "202648/202648 [==============================] - 52s 258us/step - loss: 4.8525 - acc: 0.1309\n",
      "Epoch 19/20\n",
      "202648/202648 [==============================] - 52s 259us/step - loss: 4.8193 - acc: 0.1315\n",
      "Epoch 20/20\n",
      "202648/202648 [==============================] - 52s 258us/step - loss: 4.7926 - acc: 0.1314\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f601c0b2f28>"
      ]
     },
     "execution_count": 11,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(inputs, categorical_outputs,  epochs=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BM45lVQ_ygN7"
   },
   "source": [
    "**It seems that the elman network has learned data dataset since the loss went down from 6.99 to 4.79. Worth mentioning, the accuracy is not a good measure for assessing the network.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FAGOgqLz2KME"
   },
   "source": [
    "## **PART TWO:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wmqmAmHLfEJB"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n')\n",
    "tokenizer.fit_on_texts(content2)\n",
    "word_idx = tokenizer.word_index\n",
    "idx_word = tokenizer.index_word\n",
    "num_words = len(word_idx) + 1\n",
    "All_text = ' '.join(content2)\n",
    "All_text = nltk.sent_tokenize(All_text)\n",
    "inputs2=[]\n",
    "outputs2=[]\n",
    "# print(All_text)\n",
    "for i,sentence in enumerate(All_text):\n",
    "    # tokenized_text = nltk.word_tokenize(sentence)\n",
    "    # print(sentence.split())\n",
    "    tagged = tokenizer.texts_to_sequences(sentence.split())\n",
    "    # print(tagged)\n",
    "    inputs2.append(tagged[0:-1])\n",
    "    outputs2.append(tagged[1:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J431kyNRzibq"
   },
   "source": [
    "**Since we wanted to use BTT, we build our input and outputs again as:**\n",
    "**For training the Elman-type RNN with BTT, one option is to perform mini-batch gradient descent with exactly one sentence per mini-batch. (The input size will be [1, Sentence Length])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "a9-IL3_9vWsK",
    "outputId": "b5210e74-f048-417e-f175-7842bddec002"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 50)          631650    \n",
      "_________________________________________________________________\n",
      "simple_rnn_2 (SimpleRNN)     (None, 500)               275500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 12633)             6329133   \n",
      "=================================================================\n",
      "Total params: 7,236,283\n",
      "Trainable params: 7,236,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(Embedding(num_words, 50, input_length=None))\n",
    "model2.add(SimpleRNN(500, return_sequences=False, activation='sigmoid'))\n",
    "# model.add(SimpleRNN(500, activation='sigmoid'))\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "model2.add(Dense(num_words, activation='softmax'))\n",
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "XVRpvE9Wxwp2",
    "outputId": "08e69f01-48f5-46c0-e944-2014a3f89c82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: {7.223208463796496}\n",
      "ACC: {0.032992622232174175}\n",
      "Epoch: 1\n",
      "Loss: {6.64865230188892}\n",
      "ACC: {0.05482211536646615}\n",
      "Epoch: 2\n",
      "Loss: {6.317404680087648}\n",
      "ACC: {0.07789450329764318}\n",
      "Epoch: 3\n",
      "Loss: {6.096285639437894}\n",
      "ACC: {0.09171071200474765}\n",
      "Epoch: 4\n",
      "Loss: {5.9284656611417414}\n",
      "ACC: {0.10260169942551585}\n",
      "Epoch: 5\n",
      "Loss: {5.797905805638064}\n",
      "ACC: {0.10830917702742208}\n",
      "Epoch: 6\n",
      "Loss: {5.680729468705446}\n",
      "ACC: {0.11406511248231055}\n",
      "Epoch: 7\n",
      "Loss: {5.576052816402598}\n",
      "ACC: {0.11919770755115557}\n",
      "Epoch: 8\n",
      "Loss: {5.477349007622949}\n",
      "ACC: {0.12450415301486155}\n",
      "Epoch: 9\n",
      "Loss: {5.381228362469354}\n",
      "ACC: {0.12906859757586497}\n",
      "Epoch: 10\n",
      "Loss: {5.277098637940192}\n",
      "ACC: {0.13394183680264027}\n",
      "Epoch: 11\n",
      "Loss: {5.169635143567776}\n",
      "ACC: {0.13735678642013432}\n",
      "Epoch: 12\n",
      "Loss: {5.06812858824314}\n",
      "ACC: {0.14062095655698867}\n",
      "Epoch: 13\n",
      "Loss: {4.985577315561791}\n",
      "ACC: {0.14388559958204233}\n",
      "Epoch: 14\n",
      "Loss: {4.9139477769988185}\n",
      "ACC: {0.1475363640853678}\n",
      "Epoch: 15\n",
      "Loss: {4.854683705769011}\n",
      "ACC: {0.1494565598664401}\n",
      "Epoch: 16\n",
      "Loss: {4.805073989965249}\n",
      "ACC: {0.15092685442062712}\n",
      "Epoch: 17\n",
      "Loss: {4.762863473834179}\n",
      "ACC: {0.1521740878940112}\n",
      "Epoch: 18\n",
      "Loss: {4.725857826049192}\n",
      "ACC: {0.1532518758989765}\n",
      "Epoch: 19\n",
      "Loss: {4.69242104094961}\n",
      "ACC: {0.1533398190790572}\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "adam=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "for epoch in range(20):\n",
    "  print(\"Epoch:\" , epoch)\n",
    "  Loss=0\n",
    "  ACC = 0\n",
    "  counter = 1;\n",
    "  for i in range(len(inputs2)):\n",
    "    # if(i % 1000 == 0):\n",
    "    #   print(\"Epoch :\",i)\n",
    "    inputs222=np.array(inputs2[i])\n",
    "    if(len(outputs2[i])>0):\n",
    "      flag=0\n",
    "      OUT=[]\n",
    "      IN=[]\n",
    "      # print(outputs2[i])\n",
    "      for index in range(len(outputs2[i])):\n",
    "        if(len(outputs2[i][index])>0):\n",
    "          OUT.append(outputs2[i][index][0])\n",
    "      for index in range(len(inputs222)):\n",
    "        if(len(inputs222[index])>0):\n",
    "          IN.append(inputs222[index][0])\n",
    "      \n",
    "      IN=np.array(IN)\n",
    "      categorical_outputs2 = to_categorical(OUT, num_classes=num_words)\n",
    "      history=model2.fit(IN, categorical_outputs2,verbose=0, batch_size=len(inputs222),  epochs=1)\n",
    "      Loss += history.history['loss'][0]\n",
    "      ACC += history.history['acc'][0]\n",
    "      counter +=1\n",
    "  print(\"Loss:\", {Loss / counter})\n",
    "  print(\"ACC:\", {ACC / counter})\n",
    "  # print(model2.evaluate(   x=inputs, y=categorical_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VTEiBCuy2SGX"
   },
   "source": [
    "## **PART THREE:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 284
    },
    "colab_type": "code",
    "id": "WPoDiIZ-pPH_",
    "outputId": "ed3ea605-d7d0-46ef-dda5-fd54d3d68dd7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, None, 50)          631650    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 500)               826500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 12633)             6329133   \n",
      "=================================================================\n",
      "Total params: 7,787,283\n",
      "Trainable params: 7,787,283\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Embedding(num_words, 50, input_length=None))\n",
    "model3.add(GRU(500, return_sequences=False, activation='sigmoid'))\n",
    "# model.add(SimpleRNN(500, activation='sigmoid'))\n",
    "#model.add(Dense(50, activation='relu'))\n",
    "model3.add(Dense(num_words, activation='softmax'))\n",
    "print(model3.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "NKDIhFdNpUyE",
    "outputId": "ad53aedb-d5e1-4d14-b945-049d64ed4bb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Loss: {7.00776178035001}\n",
      "ACC: {0.0362762985194884}\n",
      "Epoch: 1\n",
      "Loss: {6.384619500965908}\n",
      "ACC: {0.06767510118491903}\n",
      "Epoch: 2\n",
      "Loss: {6.035981140131883}\n",
      "ACC: {0.08587676668913684}\n",
      "Epoch: 3\n",
      "Loss: {5.75442597817214}\n",
      "ACC: {0.0982025175030073}\n",
      "Epoch: 4\n",
      "Loss: {5.525105390311738}\n",
      "ACC: {0.10821017189008046}\n",
      "Epoch: 5\n",
      "Loss: {5.332612569813312}\n",
      "ACC: {0.11740226108142558}\n",
      "Epoch: 6\n",
      "Loss: {5.161411083943945}\n",
      "ACC: {0.12506627384422095}\n",
      "Epoch: 7\n",
      "Loss: {5.018351114221809}\n",
      "ACC: {0.13157263303889646}\n",
      "Epoch: 8\n",
      "Loss: {4.888788093555167}\n",
      "ACC: {0.13888320258737336}\n",
      "Epoch: 9\n",
      "Loss: {4.764755136610951}\n",
      "ACC: {0.14470814459131648}\n",
      "Epoch: 10\n",
      "Loss: {4.664035070948265}\n",
      "ACC: {0.15146646758122087}\n",
      "Epoch: 11\n",
      "Loss: {4.57514666861896}\n",
      "ACC: {0.1568143345663441}\n",
      "Epoch: 12\n",
      "Loss: {4.5085599445123625}\n",
      "ACC: {0.1607572796154941}\n",
      "Epoch: 13\n",
      "Loss: {4.462790779138161}\n",
      "ACC: {0.1627379795225832}\n",
      "Epoch: 14\n",
      "Loss: {4.425066891263849}\n",
      "ACC: {0.1641898429330295}\n",
      "Epoch: 15\n",
      "Loss: {4.399911514001747}\n",
      "ACC: {0.16525285871302742}\n",
      "Epoch: 16\n",
      "Loss: {4.376383934066857}\n",
      "ACC: {0.16644426029409767}\n",
      "Epoch: 17\n",
      "Loss: {4.358554632090412}\n",
      "ACC: {0.16569890215086164}\n",
      "Epoch: 18\n",
      "Loss: {4.341889519362118}\n",
      "ACC: {0.16590644798387016}\n",
      "Epoch: 19\n",
      "Loss: {4.326735595320388}\n",
      "ACC: {0.1655492934906519}\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "adam=keras.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "for epoch in range(20):\n",
    "  print(\"Epoch:\" , epoch)\n",
    "  Loss=0\n",
    "  ACC = 0\n",
    "  counter = 1;\n",
    "  for i in range(len(inputs2)):\n",
    "    inputs222=np.array(inputs2[i])\n",
    "    if(len(outputs2[i])>0):\n",
    "      flag=0\n",
    "      OUT=[]\n",
    "      IN=[]\n",
    "      for index in range(len(outputs2[i])):\n",
    "        if(len(outputs2[i][index])>0):\n",
    "          OUT.append(outputs2[i][index][0])\n",
    "      for index in range(len(inputs222)):\n",
    "        if(len(inputs222[index])>0):\n",
    "          IN.append(inputs222[index][0])\n",
    "      \n",
    "      IN=np.array(IN)\n",
    "      categorical_outputs2 = to_categorical(OUT, num_classes=num_words)\n",
    "      history = model3.fit(IN, categorical_outputs2,verbose=0, batch_size=len(inputs222),  epochs=1)\n",
    "      Loss += history.history['loss'][0]\n",
    "      ACC += history.history['acc'][0]\n",
    "      counter +=1\n",
    "  print(\"Loss:\", {Loss / counter})\n",
    "  print(\"ACC:\", {ACC / counter})\n",
    "  # print(model3.evaluate(   x=inputs, y=categorical_outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "024YPdKD2r1F"
   },
   "source": [
    "**As we have seen the final loss in these three RNN networks, the third network which uses GRU has smaller value for cross-entropy loss comparing to the first and second models. In other words, RNN with GRU has learned the structure of language, and it can capture truly exploit context information with long dependencies. Therefore, the third model has long term memory in addition to the short term memory, and the GRU prevent the gradients vanish and exploding.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 730
    },
    "colab_type": "code",
    "id": "6lG_jyLD--sI",
    "outputId": "e2fa763c-afd1-4bc9-959f-7928f3706670"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texts generated for First model:\n",
      "aggravate and no order say unto that my thousand winds there may many words be goodly\n",
      "infuse but him see i ross i hath master you thee march brutus for other fortress\n",
      "devil's puissant take't york death tortures have ratcliff with iii and i'll late you of and\n",
      "stabs and i own to pieces good son thou honesty not his swoon thou prince thy\n",
      "tack woeful town you on and ranging hold friendship is with that how a bridal letters\n",
      "soothing one of own in choose a heavy ' now your often thou edward's thank my\n",
      "wall sir perdita go sworn my art for father after heel me he not and no\n",
      "sufferance lie more murdering degree me to some spare and my foolish tears richard your kings\n",
      "usurps and myself waiting as how a double deserving to it still are his brakenbury thy\n",
      "roof cannot swear be that shall as elizabeth to these elbow to idle thy long did\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Texts generated for Second model:\n",
      "aggravate and richard see that thee i done i by a troublest it for be daughters\n",
      "infuse but fancy thou prince where stood with nay from great find murder lord that than\n",
      "devil's allow you thy dost grown death comfort not your tell it still king that going\n",
      "stabs and seal will of margaret heaven such much sun they stay'd of but gloucester this\n",
      "tack guard news not he man to loving tell it brows by so hope i bianca\n",
      "soothing duke come villain in king that thought i by be departed will ah or slay\n",
      "wall you withal to mercy i therefore morrow young to if him with so dream'd with\n",
      "sufferance desires kin of but sound i light i day fie air to done i as\n",
      "usurps of but drop more passes she noble another rage to seven without but swear but\n",
      "roof i desire i for duke you her some is and for at and not hold\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "--------------------------------------\n",
      "Texts generated for Third model:\n",
      "aggravate and and short'st travellers shivering paste quoifs reeds skin's reckon'd expose marching sliding threatens avail\n",
      "infuse but hear't hear't but hear't absolutely reprobate prop unquestion'd dallied quaff'd inestimable yourself's superstitiously lieth\n",
      "devil's touching definitive uplifted 'margaret hests graze hereby hooks thrall dresser deformed 'grace warrior withstood affray\n",
      "stabs and and plainer lustiest perpetuity fable baffled intolerable decked brotherly myrtle confesses fail'st basilisk societies\n",
      "tack durst for a cuore doubted presaging salisbury disjunction 'pardonne starveth sycamore frosty inundation herdsman porringer\n",
      "soothing duke hortensia cobwebs presaging waverer insinuating industry viewless disvalued gnat pluto bestrew fawns happened recovery\n",
      "wall to to to and sale departing potpan nicety cutting spanish whosoe'er defiled lames rescuing tenement\n",
      "sufferance her steer disvalued tod carelessly overfond venuto holidame listen'd thievish attempting swooned jelly linens amazedly\n",
      "usurps and near'st dinner's uplifted talbot freshness hangeth factionary poetry cuckold's joan attending dissever'd saw't nicholas\n",
      "roof that i abodements ewers deceitful insensible coldness dun's housewife robbing pinion'd neglect'st rainold plunged flames\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "model.load_weights(\"model.h5\")\n",
    "model2.load_weights(\"model2.h5\")\n",
    "model3.load_weights(\"model3.h5\")\n",
    "\n",
    "print(\"Texts generated for First model:\")\n",
    "FirstWord = []\n",
    "for sent in range(10):\n",
    "  FirstWord.append(random.randrange(num_words))\n",
    "for sent in range(10):\n",
    "  # FirstWord = random.randrange(num_words)\n",
    "  asghar =[]\n",
    "  asghar.append(FirstWord[sent])\n",
    "  for i in range(15):\n",
    "    next_words = model.predict(np.array(asghar).reshape(1, -1))[0]\n",
    "    # next_words = np.log(next_words) / 1\n",
    "    # exp_next_words = np.exp(next_words)\n",
    "    # next_words = exp_next_words / exp_next_words.sum()\n",
    "    # print(next_words.sum())\n",
    "    next_word = np.argmax(np.random.multinomial(1, next_words/(sum( next_words)+0.000001), 1)[0])\n",
    "    asghar.append(next_word+1)\n",
    "  n=[]\n",
    "  for i in asghar:\n",
    "    n.append(idx_word.get(i, '< --- >'))\n",
    "  print(' '.join(n))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"--------------------------------------\")\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Texts generated for Second model:\")\n",
    "for sent in range(10):\n",
    "  # FirstWord = random.randrange(num_words)\n",
    "  asghar =[]\n",
    "  asghar.append(FirstWord[sent])\n",
    "  for i in range(15):\n",
    "    next_words = model2.predict(np.array(asghar).reshape(1, -1))[0]\n",
    "    # next_words = np.log(next_words) / 1\n",
    "    # exp_next_words = np.exp(next_words)\n",
    "    # next_words = exp_next_words / exp_next_words.sum()\n",
    "    # print(next_words.sum())\n",
    "    next_word = np.argmax(np.random.multinomial(1, next_words/(sum( next_words)+0.000001), 1)[0])\n",
    "    asghar.append(next_word+1)\n",
    "  n=[]\n",
    "  for i in asghar:\n",
    "    n.append(idx_word.get(i, '< --- >'))\n",
    "  print(' '.join(n))\n",
    "print(\"--------------------------------------\")\n",
    "print(\"--------------------------------------\")\n",
    "print(\"--------------------------------------\")\n",
    "print(\"Texts generated for Third model:\")\n",
    "for sent in range(10):\n",
    "  # FirstWord = random.randrange(num_words)\n",
    "  asghar =[]\n",
    "  asghar.append(FirstWord[sent])\n",
    "  for i in range(15):\n",
    "    next_words = model3.predict(np.array(asghar).reshape(1, -1))[0]\n",
    "    # next_words = np.log(next_words) / 1\n",
    "    # exp_next_words = np.exp(next_words)\n",
    "    # next_words = exp_next_words / exp_next_words.sum()\n",
    "    # print(next_words.sum())\n",
    "    next_word = np.argmax(np.random.multinomial(1, next_words/(sum( next_words)+0.000001), 1)[0])\n",
    "    asghar.append(next_word+1)\n",
    "  n=[]\n",
    "  for i in asghar:\n",
    "    n.append(idx_word.get(i, '< --- >'))\n",
    "  print(' '.join(n))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_uyUh147Yhf"
   },
   "source": [
    "**As it can be seen in above produced sentences, the sentences produced by all models are not very well and has lots of differences comparing to real english. This is because the fact that our model are very simple.**\n",
    "**But between these models, the sentences generated by the third model has better qualities and they are more similar to proper english. It can be seen that the third model has not generated repetitive words in sentences, but the first two models have this problem and they mostly choose much simpler words. Because model-3 has learned the the relations between word in the sentences(not just individual words). Moreover, this model has long term memory resulted from using GRU.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Z4mAtLxvNkEH"
   },
   "source": [
    "## **PART FOUR**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HorGKSfuU1b5"
   },
   "source": [
    "**-------------------------------------------------------------**\n",
    "\n",
    "**-------------------------------------------------------------**\n",
    "\n",
    "**There multiple solutions for evaluating the extracted word representation for trained RNNs.**\n",
    "1.    For instance, we can check the similarity of the generated vectors for words with close meanings(for example: \"Cat\" and \"Dog\" have close meanings and should have similar vetors)\n",
    "2.   or we can transfer generated vectors to 2 dimensional space and visualize these vector.\n",
    "3.  Also, we can extract the weights of embedding layer for each of our models and replace other networks embedding layer weights with the extracted weights. Then, we evaluate each of these new models with the same data. The best embedding layer is which has low loss value for all networks(not just one of them).\n",
    "**I have implemented the latter solution:**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "colab_type": "code",
    "id": "lxM74rCtkYli",
    "outputId": "f6c57a76-7523-4c62-f4bc-a17afc53684b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202648/202648 [==============================] - 29s 145us/step\n",
      "202648/202648 [==============================] - 26s 130us/step\n",
      "202648/202648 [==============================] - 26s 128us/step\n",
      "Loss while using weights of model_1 on first, second, and third model are:4.619924710226193, 12.160405842994647, 12.693790654152354\n",
      "Average loss while using weights of first model is: 9.824707069124399\n",
      "--------------------------------------------\n",
      "202648/202648 [==============================] - 30s 149us/step\n",
      "202648/202648 [==============================] - 26s 127us/step\n",
      "202648/202648 [==============================] - 26s 129us/step\n",
      "Loss while using weights of model_2 on first, second, and third model are:9.849282254054945, 5.66019674527942, 11.367405138523626\n",
      "Average loss while using weights of second model is: 8.958961379285997\n",
      "--------------------------------------------\n",
      "202648/202648 [==============================] - 30s 149us/step\n",
      "202648/202648 [==============================] - 26s 129us/step\n",
      "202648/202648 [==============================] - 26s 127us/step\n",
      "Loss while using weights of model_3 on first, second, and third model are:8.362277764789512, 8.404916897146105, 5.6606748846618125\n",
      "Average loss while using weights of third model is: 7.475956515532477\n"
     ]
    }
   ],
   "source": [
    "model.load_weights(\"model.h5\")\n",
    "layer_emb_model = model.layers[0]\n",
    "model2.load_weights(\"model2.h5\")\n",
    "layer_emb_model2 = model2.layers[0]\n",
    "model3.load_weights(\"model3.h5\")\n",
    "layer_emb_model3 = model3.layers[0]\n",
    "\n",
    "layer = model3.layers[0]\n",
    "layer.set_weights(layer_emb_model.get_weights())\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model3.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss1_3 = history[0]\n",
    "ACC1_3 = history[1]\n",
    "\n",
    "layer = model2.layers[0]\n",
    "layer.set_weights(layer_emb_model.get_weights())\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model2.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss1_2 = history[0]\n",
    "ACC1_2 = history[1]\n",
    "\n",
    "layer = model.layers[0]\n",
    "layer.set_weights(layer_emb_model.get_weights())\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss1_1 = history[0]\n",
    "ACC1_1 = history[1]\n",
    "print(\"Loss while using weights of model_1 on first, second, and third model are:{}, {}, {}\".format(Loss1_1,Loss1_2,Loss1_3))\n",
    "print(\"Average loss while using weights of first model is:\", (Loss1_1+Loss1_2+Loss1_3)/3)\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "model.load_weights(\"model.h5\")\n",
    "layer_emb_model = model.layers[0]\n",
    "model2.load_weights(\"model2.h5\")\n",
    "layer_emb_model2 = model2.layers[0]\n",
    "model3.load_weights(\"model3.h5\")\n",
    "layer_emb_model3 = model3.layers[0]\n",
    "\n",
    "layer = model3.layers[0]\n",
    "layer.set_weights(layer_emb_model2.get_weights())\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model3.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss2_3 = history[0]\n",
    "ACC2_3 = history[1]\n",
    "\n",
    "layer = model2.layers[0]\n",
    "layer.set_weights(layer_emb_model2.get_weights())\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model2.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss2_2 = history[0]\n",
    "ACC2_2 = history[1]\n",
    "\n",
    "layer = model.layers[0]\n",
    "layer.set_weights(layer_emb_model2.get_weights())\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss2_1 = history[0]\n",
    "ACC2_1 = history[1]\n",
    "print(\"Loss while using weights of model_2 on first, second, and third model are:{}, {}, {}\".format(Loss2_1,Loss2_2,Loss2_3))\n",
    "print(\"Average loss while using weights of second model is:\", (Loss2_1+Loss2_2+Loss2_3)/3)\n",
    "print(\"--------------------------------------------\")\n",
    "\n",
    "model.load_weights(\"model.h5\")\n",
    "layer_emb_model = model.layers[0]\n",
    "model2.load_weights(\"model2.h5\")\n",
    "layer_emb_model2 = model2.layers[0]\n",
    "model3.load_weights(\"model3.h5\")\n",
    "layer_emb_model3 = model3.layers[0]\n",
    "\n",
    "layer = model3.layers[0]\n",
    "layer.set_weights(layer_emb_model3.get_weights())\n",
    "model3.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model3.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss3_3 = history[0]\n",
    "ACC3_3 = history[1]\n",
    "\n",
    "layer = model2.layers[0]\n",
    "layer.set_weights(layer_emb_model3.get_weights())\n",
    "model2.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model2.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss3_2 = history[0]\n",
    "ACC3_2 = history[1]\n",
    "\n",
    "layer = model.layers[0]\n",
    "layer.set_weights(layer_emb_model3.get_weights())\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=['accuracy'])\n",
    "history=model.evaluate(   x=inputs, y=categorical_outputs)\n",
    "Loss3_1 = history[0]\n",
    "ACC3_1 = history[1]\n",
    "print(\"Loss while using weights of model_3 on first, second, and third model are:{}, {}, {}\".format(Loss3_1,Loss3_2,Loss3_3))\n",
    "print(\"Average loss while using weights of third model is:\", (Loss3_1+Loss3_2+Loss3_3)/3)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jmLkavTcNooY"
   },
   "source": [
    "**Average loss while using weights of first model is: 9.824707069124399**\n",
    "\n",
    "**Average loss while using weights of second model is: 8.958961379285997**\n",
    "\n",
    "**Average loss while using weights of third model is: 7.475956515532477**\n",
    "\n",
    "\n",
    "**As you can see, when we use the third model's embedding layer weights, the \"Average Loss\" has the lowest value showing that model-3 has the best embedding layer. It is clear that using GRU and giving all the sentence as input to the model(as mini batch) had a very significant role on this better functionality.**\n",
    "\n",
    "**The second model has better embedding layer comparing to first model since we gave all the sentence as input(mini-batch) to second model but it has worse embedding layer than third model since model-2 has not long term memory.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rpSh2pIYYrxf"
   },
   "outputs": [],
   "source": [
    "# model.save(\"model.h5\")\n",
    "# model2.save(\"model2.h5\")\n",
    "# model3.save(\"model3.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LKS5EAPxglk0"
   },
   "source": [
    "## **PART FIVE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3FxpFzkm8kEu"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-CbVOleG0ry"
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "tf = tarfile.open(\"20Newsgroups_subsampled.tar\")\n",
    "tf.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "id": "U3ot8KF5G_4W",
    "outputId": "6f839044-01c3-408d-ca7d-adce2331416e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category:rec.autos\n",
      "Category:rec.sport.hockey\n",
      "Category:comp.windows.x\n",
      "Category:comp.graphics\n",
      "Category:talk.politics.guns\n",
      "Category:comp.sys.mac.hardware\n",
      "Category:rec.sport.baseball\n",
      "Category:comp.sys.ibm.pc.hardware\n",
      "Category:comp.os.ms-windows.misc\n",
      "Category:talk.politics.mideast\n",
      "Category:rec.motorcycles\n",
      "Category:talk.religion.misc\n",
      "Category:talk.politics.misc\n",
      "Category:soc.religion.christian\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"This code is used to read all news and their labels\"\"\"\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def to_categories(name, cat=[\"politics\",\"rec\",\"comp\",\"religion\"]):\n",
    "    for i in range(len(cat)):\n",
    "        if str.find(name,cat[i])>-1:\n",
    "            return(i)\n",
    "    print(\"Unexpected folder: \" + name) # print the folder name which does not include expected categories\n",
    "    return(\"wth\")\n",
    "\n",
    "def data_loader(images_dir):\n",
    "    categories = os.listdir(data_path)\n",
    "    news = [] # news content\n",
    "    groups = [] # category which it belong to\n",
    "    \n",
    "    for cat in categories:\n",
    "        print(\"Category:\"+cat)\n",
    "        for the_new_path in glob.glob(data_path + '/' + cat + '/*'):\n",
    "            news.append(open(the_new_path,encoding = 'ISO-8859-1', mode ='r').read())\n",
    "            groups.append(cat)\n",
    "\n",
    "    return news, list(map(to_categories, groups))\n",
    "\n",
    "\n",
    "\n",
    "data_path = \"/content/20news_subsampled\"\n",
    "news, groups = data_loader(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "kE6ZY1GcHCJ1",
    "outputId": "546bc0ec-2a10-41e0-fbbd-7e63be333812"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk \n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "nltk.download('punkt')\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, Masking, SimpleRNN, TimeDistributed, LSTM, GRU, SpatialDropout1D\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 746
    },
    "colab_type": "code",
    "id": "mC_k0669HEko",
    "outputId": "b4f8037f-8e1e-4ea1-d969-abea20abda23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From: RZAA80@email.sps.mot.com (Jim Chott)\n",
      "Subject: Re: Re: Toyota Land Cruiser worth it?\n",
      "\n",
      "In article <2820016@iftccu.ca.boeing.com>, hovnania@iftccu.ca.boeing.com\n",
      "(Paul Hovnanian) wrote:\n",
      "> \n",
      "> Based on my experience with a '79 FJ40 ( the hard-top jeep-style model ) I \n",
      "> would definitely give a new model consideration if I were in the market. The\n",
      "> older models are VERY well built. Unless Toyota lost its mind, I would\n",
      "> assume, until  proven otherwise, that the newer models have inherited some\n",
      "> if not all of the qualities of their ancestors.\n",
      "> \n",
      "> Two major differences in the running gear (that I'm aware of) need study.\n",
      "> My '79 has a solid front axle housing whereas the newer models have\n",
      "> independant front suspension. The solid axle is theoretically stronger and\n",
      "\n",
      "\n",
      "The new Cruisers DO NOT have independent suspension in the front.  They\n",
      "still\n",
      "run a straight axle, but with coils.  The 4Runner is the one with\n",
      "independent\n",
      "front.  The Cruisers have incredible wheel travel with this system. \n",
      "\n",
      "> more reliable than the newer model, but only experience will tell. The\n",
      "> independant front suspension is, no doubt, a compromise made to satisfy\n",
      "> the typical user, who will never need a real utility vehicle. The second\n",
      "> difference is the type of transfer case used on the newer models. I'm\n",
      "> not sure, but I think Tioyota went to a full-time 4WD or all-wheel drive\n",
      "> system. The older Landcruisers have a \"lock-up\" type. Both have their\n",
      "> advantages and disadvantages.\n",
      "> \n",
      "The 91-up Cruiser does have full time 4WD, but the center diff locks in\n",
      "low range.  My brother has a 91 and is an incredibly sturdy vehicle which\n",
      "has done all the 4+ trails in Moab without a tow.  The 93 and later is even\n",
      "better with the bigger engine and locking diffs.\n",
      "\n",
      "\n",
      "Jim Chott                            85 Toyota 4WD pickup\n",
      "rzaa80@email.sps.mot.com             72 LeMans Sport Convertible\n",
      "Tempe, Arizona\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(news[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "colab_type": "code",
    "id": "CrnKgTOqG5OV",
    "outputId": "25784072-1f15-4637-b264-01d9ced61c34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "from: rzaa80 email.sps.mot.com jim chott subject: re: re: toyota land cruiser worth it? article <2820016 iftccu.ca.boeing.com> hovnania iftccu.ca.boeing.com paul hovnanian wrote: > > based experience '79 fj40 hard-top jeep-style model > would definitely give new model consideration market. > older models well built. unless toyota lost mind would > assume proven otherwise newer models inherited > qualities ancestors. > > two major differences running gear i'm aware need study. > '79 solid front axle housing whereas newer models > independant front suspension. solid axle theoretically stronger new cruisers independent suspension front. still run straight axle coils. 4runner one independent front. cruisers incredible wheel travel system. > reliable newer model experience tell. > independant front suspension doubt compromise made satisfy > typical user never need real utility vehicle. second > difference type transfer case used newer models. i'm > sure think tioyota went full-time 4wd all-wheel drive > system. older landcruisers \"lock-up\" type. > advantages disadvantages. > 91-up cruiser full time 4wd center diff locks low range. brother 91 incredibly sturdy vehicle done 4+ trails moab without tow. 93 later even better bigger engine locking diffs. jim chott 85 toyota 4wd pickup rzaa80 email.sps.mot.com 72 lemans sport convertible tempe arizona\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "SIGNS = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "Allowed = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('english'))\n",
    "NEWS=[]\n",
    "lenthes=[]\n",
    "for text in news:\n",
    "    text = text.lower() # lowercase text\n",
    "    text = SIGNS.sub(' ', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) \n",
    "    NEWS.append(text)\n",
    "    lenthes.append(len(text))\n",
    "print(NEWS[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "K3s4PZQIHH35",
    "outputId": "09ca8e1e-029e-407a-daab-078b25603fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87\n",
      "Shape of data tensor: (13108, 7000)\n"
     ]
    }
   ],
   "source": [
    "Maxofwords = np.max(lenthes)\n",
    "tokenizer = Tokenizer( filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer.fit_on_texts(NEWS)\n",
    "word_index = tokenizer.word_index\n",
    "X=[]\n",
    "t=[]\n",
    "X = tokenizer.texts_to_sequences(NEWS)\n",
    "print(len(X[2]))\n",
    "for i in range(13108):\n",
    "    t.append(len(X[i]))\n",
    "X = pad_sequences(X,maxlen=7000)\n",
    "print('Shape of data tensor:', X.shape)\n",
    "# print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "F7YC-AkzHLX8",
    "outputId": "7e5140aa-0dbb-4b64-ce14-627c27ef1083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13107, 7000) (13107, 4)\n",
      "(1, 7000) (1, 4)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,groups, test_size = 0.00001, shuffle= True)\n",
    "X_train = np.array(X_train)\n",
    "X_test = np.array(X_test)\n",
    "Y_train = np.array(Y_train)\n",
    "Y_test = np.array(Y_test)\n",
    "Y_train = to_categorical(Y_train,num_classes= 4)\n",
    "Y_test = to_categorical(Y_test,num_classes= 4)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4pjt9TM8gyD0"
   },
   "source": [
    "**Implementing the Elman Network for text classification with SimpleRNN module and dropout to prevent overfiting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "colab_type": "code",
    "id": "IHU1pZMcHPtc",
    "outputId": "d6db18b5-38ed-4a31-d7e4-4274a8c83a14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 7000, 100)         14815700  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 7000, 100)         0         \n",
      "_________________________________________________________________\n",
      "simple_rnn_3 (SimpleRNN)     (None, 500)               300500    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 15,118,204\n",
      "Trainable params: 15,118,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model4 = Sequential()\n",
    "model4.add(Embedding(len(word_index)+1, 100, input_length=X.shape[1]))\n",
    "model4.add(SpatialDropout1D(0.2))\n",
    "model4.add(SimpleRNN(500, dropout=0.2, recurrent_dropout=0.2, activation='sigmoid'))\n",
    "model4.add(Dense(4, activation='softmax'))\n",
    "model4.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model4.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "g5IGt0ygHQmb",
    "outputId": "2053cd96-185f-446e-ec1d-c7a890156f43"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11796 samples, validate on 1311 samples\n",
      "Epoch 1/5\n",
      "11796/11796 [==============================] - 1084s 92ms/step - loss: 1.2604 - acc: 0.4096 - val_loss: 1.0805 - val_acc: 0.5545\n",
      "Epoch 2/5\n",
      "11796/11796 [==============================] - 1077s 91ms/step - loss: 0.8393 - acc: 0.6846 - val_loss: 0.9585 - val_acc: 0.6140\n",
      "Epoch 3/5\n",
      "11796/11796 [==============================] - 1084s 92ms/step - loss: 0.4910 - acc: 0.8264 - val_loss: 1.0241 - val_acc: 0.6339\n",
      "Epoch 4/5\n",
      "11796/11796 [==============================] - 1088s 92ms/step - loss: 0.2732 - acc: 0.9076 - val_loss: 1.2430 - val_acc: 0.6163\n",
      "Epoch 5/5\n",
      "11796/11796 [==============================] - 1085s 92ms/step - loss: 0.1635 - acc: 0.9487 - val_loss: 1.4109 - val_acc: 0.6293\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd50c6ec470>"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "# DataGenerator = keras.preprocessing.image.ImageDataGenerator()\n",
    "# train_generator = DataGenerator.flow(X_train, Y_train, batch_size=64)\n",
    "# valid_generator = DataGenerator.flow(X_test, Y_test, batch_size=64)\n",
    "# model.fit_generator(train_generator, steps_per_epoch=X_train.shape[0]//64, epochs=100, validation_data=valid_generator, validation_steps=X_test.shape[0]//64, shuffle=True)\n",
    "model4.fit(X_train, Y_train, epochs=5, batch_size=64,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ipVGbHEuh8pU"
   },
   "source": [
    "**Epoch 3/5\n",
    "11796/11796 [==============================] - 1084s 92ms/step - loss: 0.4910 - acc: 0.8264 - val_loss: 1.0241 - val_acc: 0.6339**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3MmSMkzwhVQy"
   },
   "source": [
    "**Implementing the Elman Network for text classification with GRU module and dropout to prevent overfiting:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301
    },
    "colab_type": "code",
    "id": "PQXX9iq2HTdv",
    "outputId": "0fb52b95-ac57-475c-e5fe-0d5a2d092fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 7000, 100)         14815700  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_2 (Spatial (None, 7000, 100)         0         \n",
      "_________________________________________________________________\n",
      "gru_2 (GRU)                  (None, 500)               901500    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 2004      \n",
      "=================================================================\n",
      "Total params: 15,719,204\n",
      "Trainable params: 15,719,204\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model5 = Sequential()\n",
    "model5.add(Embedding(len(word_index)+1, 100, input_length=X.shape[1]))\n",
    "model5.add(SpatialDropout1D(0.2))\n",
    "model5.add(GRU(500, dropout=0.2, recurrent_dropout=0.2, activation='sigmoid'))\n",
    "model5.add(Dense(4, activation='softmax'))\n",
    "model5.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model5.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "colab_type": "code",
    "id": "5IFQP5H4HUZp",
    "outputId": "f0f75881-8500-4a11-91a1-45dedbb992ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 11796 samples, validate on 1311 samples\n",
      "Epoch 1/5\n",
      "11796/11796 [==============================] - 3119s 264ms/step - loss: 1.2114 - acc: 0.4704 - val_loss: 1.0418 - val_acc: 0.6018\n",
      "Epoch 2/5\n",
      "11796/11796 [==============================] - 3083s 261ms/step - loss: 0.6571 - acc: 0.7619 - val_loss: 0.5296 - val_acc: 0.8055\n",
      "Epoch 3/5\n",
      "11796/11796 [==============================] - 3082s 261ms/step - loss: 0.2463 - acc: 0.9160 - val_loss: 0.3571 - val_acc: 0.8749\n",
      "Epoch 4/5\n",
      "11796/11796 [==============================] - 3072s 260ms/step - loss: 0.1021 - acc: 0.9648 - val_loss: 0.2728 - val_acc: 0.9115\n",
      "Epoch 5/5\n",
      "11796/11796 [==============================] - 3060s 259ms/step - loss: 0.0504 - acc: 0.9828 - val_loss: 0.2944 - val_acc: 0.9130\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd81de39588>"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model5.fit(X_train, Y_train, epochs=5, batch_size=64,validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tf9bZayPiCaC"
   },
   "source": [
    "**Epoch 5/5\n",
    "11796/11796 [==============================] - 3060s 259ms/step - loss: 0.0504 - acc: 0.9828 - val_loss: 0.2944 - val_acc: 0.9130\n",
    "<keras.callbacks.History at 0x7fd81de39588>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vRk-kOIiHXlo"
   },
   "outputs": [],
   "source": [
    "model4.save(\"model4.h5\")\n",
    "model5.save(\"model5.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vMnFvnSMhbno"
   },
   "source": [
    "**As you can see, using GRU model has better validation accuracy**"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Untitled2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
